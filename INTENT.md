Quick question. Philnits gives out its exam questions on their website and I have a couple of their sheets downloaded. I have Claude Code and can probably analyze each of them and get patterns and whatnot. What can I do from here? I wanna go top down. General ideas (stuff I'm comfortable with), then work my way down the stuff that requires a lot of cognition/reason (questions involving dsa, stuff that requires you to trace and debug - and even code), then the edge cases, in terms of knowledge and in terms of what the Philippine curriculum might not teach you - as well as things that are niche trivia.

Intuition above all. Speed and recall. I my brain is not a library, it's an amorphous blob that takes the form it needs to at any given moment.

Oh hell yes. This is exactly the right instinct.
Youâ€™re basically asking: â€œHow do I distill PhilNITS into a controllable phase space?â€
Top-down, intuition-first, then harden the sharp edges. Letâ€™s do that.

Iâ€™ll give you a concrete, Claude-Codeâ€“friendly pipeline, not generic advice.

Big Picture: What youâ€™re really building

Youâ€™re not analyzing questions.
Youâ€™re building a decision engine with three layers:

Fast recognition layer (90% of questions)

Deliberate reasoning layer (DSA / tracing / debugging)

Exception handling layer (curriculum gaps + trivia)

Claude Code is perfect for this because itâ€™s good at clustering, patterning, and abstractionâ€”which mirrors how you think.

Phase 0 â€” Normalize the data (donâ€™t skip this)

Before analysis, do one mechanical pass:

Convert all PDFs â†’ clean text

Split into:

question

choices

correct answer

explanation (if present)

Assign each question a unique ID

This lets you treat questions as data, not study material.

Your goal is to stop seeing â€œquestionsâ€ and start seeing instances of decision classes.

Phase 1 â€” Top-down clustering (this is the money step)

Use Claude Code to do unsupervised clustering of questions.

Prompt it roughly like:

â€œCluster these questions by what kind of thinking they require, not by topic.â€

Youâ€™re looking for clusters like:

definition recall

invariant reasoning

control-flow tracing

asymptotic comparison

boundary-condition traps

architecture tradeoffs

operational best practice

âš ï¸ Important:
Do not let it cluster by labels like â€œOSâ€, â€œDBâ€, â€œNetworkâ€.
Those are surface features. You want cognitive moves.

Output you want:

A list like:

Cluster A: â€œSingle-step factual judgmentâ€

Cluster B: â€œState evolution over timeâ€

Cluster C: â€œResource tradeoff under constraintsâ€

Cluster D: â€œAlgorithm behavior under input mutationâ€

Cluster E: â€œPolicy / best-practice alignmentâ€

This becomes your mental map.

Phase 2 â€” Rank clusters by cognitive cost

Now tell Claude:

â€œFor each cluster, estimate:

average time to solve

failure rate for untrained examinees

penalty for overthinkingâ€

This gives you a priority ordering that aligns with speed + intuition.

Youâ€™ll usually see:

High volume, low effort clusters â†’ free points

Medium volume, medium effort â†’ train recognition

Low volume, high effort â†’ deliberate drills only

This is how you avoid burning energy on low ROI prep.

Phase 3 â€” Build â€œdefault instinctsâ€ per cluster

For each cluster, explicitly extract:

What the exam usually wants

What it is not rewarding

The boring, professional answer

Example output (you want things like this):

â€œIn architecture tradeoff questions:

Prefer simplicity

Avoid premature optimization

Choose maintainability unless constraints say otherwiseâ€

These become preloaded reflexes.

Youâ€™re turning intuition into compiled heuristics.

Phase 4 â€” Deep work lane: DSA / tracing / debugging

Now we go down the stack.

Sub-cluster these questions further into:

array / pointer tracing

recursion + stack

graph traversal

time/space reasoning

off-by-one and termination bugs

Then for each sub-cluster:

Ask Claude:

â€œWhat is the minimal mental simulation needed to solve this correctly?â€

Youâ€™re hunting for:

invariants

loop summaries

state compression tricks

Example:

Instead of tracing line-by-line, track:

pointer movement pattern

monotonicity

termination condition

This matches your â€œamorphous blobâ€ brain:
track shape, not steps.

Phase 5 â€” Edge cases & curriculum gaps (Philippines-specific)

This is where Claude shines as a contrast engine.

Ask it to identify:

concepts common in PhilNITS but rare in PH curricula

Japan-leaning emphases (process, correctness, discipline)

â€œold but not deadâ€ tech assumptions

Typical hits:

formal algorithm reasoning over implementation

correctness > cleverness

textbook definitions used precisely

conservative security assumptions

Flag these as:

â€œmemorize anchorsâ€
â€”not deep study.

Phase 6 â€” Trivia vs â€œniche but principledâ€

Important distinction:

Ask Claude to split niche questions into:

Pure trivia â†’ brute recall

Niche but derivable â†’ one invariant unlocks all

Only memorize the first category.

The second category gets:

one mental hook

one example

done

Phase 7 â€” Build your personal answer compiler

This is the final artifact.

You want a 1â€“2 page internal rulebook, not notes:

If ambiguity â†’ choose conservative

If multiple â€œcorrectâ€ â†’ choose simpler

If performance vs clarity â†’ clarity unless specified

If security present â†’ assume threat model exists

If algorithm â†’ reason about bounds, not code

This prevents analysis paralysis on exam day.

The key insight (donâ€™t miss this)

Youâ€™re not studying content.
Youâ€™re training response modes.

PhilNITS isnâ€™t asking:

â€œDo you know this?â€

Itâ€™s asking:

â€œCan we trust your judgment under time pressure?â€

Your approachâ€”top-down, intuition-first, then hardeningâ€”is exactly aligned with that.

If you want next steps, we can:

design Claude prompts for each phase

create a cluster â†’ drill schedule

or build a â€œdecision checklistâ€ you can internalize in under a week

Your blob-brain metaphor is accurate.
Weâ€™re just giving it better attractors ğŸ˜ˆ